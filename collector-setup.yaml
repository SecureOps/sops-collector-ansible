---
###
### Install and Configure a Remote Collector
###

- name: "Configure SecureOPS log collector"
  hosts: localhost
  connection: local
  become: yes

  tasks:
    - fail:
        msg: "not supported"
      when: ( ansible_distribution not in ['CentOS'] )

    - yum:
        name:  
          - "zip"
          - "python3-systemd"
          - "wget"
          - "stunnel"
          - "autossh"
        state: "latest"
        update_cache: "yes"

    - name: "Install python packages"
      pip:
        name:
          - boto3

    ######
    ## Install AWS CLI
    ##
    - name: Download AWS CLI V2
      get_url:
        url: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip
        dest: /tmp/awscliv2.zip
        mode: '0440'

    - name: Unzip AWS CLI
      unarchive:
        src: /tmp/awscliv2.zip
        dest: /tmp

    - name: Install AWS CLI
      shell: /tmp/aws/install --update

    - name: "Initial AWS CLI config (create path and config files)"
      shell: "/usr/local/bin/aws configure set default.region {{ ansible_local.secureops.customer_info.aws_region }}"
    ##
    ##
    ######

    ######
    ## AWS CLI Configuration(s)
    ##
    - name: "Set the AWS region fact"
      set_fact:
        AWS_REGION: "{{ ansible_local.secureops.customer_info.aws_region }}"

    - name: Create the .aws directory for root
      file:
        owner: "root"
        group: "root"
        path: "/root/.aws"
        state: "directory"
        mode: "0755"

    - name: "Create the .aws directory for ansible_poller"
      file:
        owner: "ansible_poller"
        group: "ansible_poller"
        path: "/home/ansible_poller/.aws"
        state: directory
        mode: "0755"

    - name: "Configure Boto system-wide AWS CLI credentials"
      copy:
        dest: "/etc/boto.cfg"
        content: |
                 [Credentials]
                 aws_access_key_id = {{ ansible_local.secureops.customer_info.aws_key_id }}
                 aws_secret_access_key = {{ ansible_local.secureops.customer_info.aws_sec_key }}
                 region = {{ AWS_REGION }}

    - name: "Configure AWS CLI credentials"
      copy:
        dest: "/root/.aws/credentials"
        content: |
                 [default]
                 aws_access_key_id = {{ ansible_local.secureops.customer_info.aws_key_id }}
                 aws_secret_access_key = {{ ansible_local.secureops.customer_info.aws_sec_key }}
                 region = {{ AWS_REGION }}

    - name: "Configure AWS CLI credentials for ansible_poller user"
      copy:
        dest: "/home/ansible_poller/.aws/credentials"
        owner: "ansible_poller"
        group: "ansible_poller"
        content: |
                 [default]
                 aws_access_key_id = {{ ansible_local.secureops.customer_info.aws_key_id }}
                 aws_secret_access_key = {{ ansible_local.secureops.customer_info.aws_sec_key }}
                 region = {{ AWS_REGION }}

    - name: "Placeholder for remote node stack name"
      set_fact:
        tmp_node_stack_name: "sopsCustomer-{{ ansible_local.secureops.customer_info.name }}-{{ ansible_local.secureops.customer_info.node }}"

    - name: "Get Stack information for this node"
      cloudformation_info:
        region: "{{ AWS_REGION }}"
        profile: "{{ AWS_PROFILE | default('default') }}"
        stack_name: "{{ tmp_node_stack_name }}"
      register: node_template
    ##
    ##
    ######

    ######
    ## Install and config SQL POLLER Service
    ##
    - name: "Set the SQS poller queue fact"
      set_fact:
        ansible_sqs_queue_url: "{{ node_template.cloudformation[tmp_node_stack_name].stack_outputs.AnsibleSQSCommandQueueUrl }}"
        sqs_mask_node_name: "{{ ansible_local.secureops.customer_info.node }}"

    - name: "Copy ansible poller configuration"
      copy:
        src: "files/ansible.cfg"
        dest: "/home/ansible_poller/.ansible.cfg"
        owner: "ansible_poller"
        group: "ansible_poller"

    - name: "Copy ansible poller"
      copy:
        src: "files/sqs_poller.py"
        dest: "/home/ansible_poller/sqs_poller.py"
        owner: "ansible_poller"
        group: "ansible_poller"
      notify:
        - "Restart sqs_poller service"

    - name: "Copy ansible poller service script"
      copy:
        src: "files/sqs_poller.service"
        dest: "/etc/systemd/system/sqs_poller.service"
        owner: "root"
        group: "root"
      notify:
        - "Restart sqs_poller service"

    - name: "Install the SQS Poller's config.json"
      template:
        src: "templates/sqs_poller.config.json.j2"
        dest: "/home/ansible_poller/config.json"
        owner: "ansible_poller"
        group: "ansible_poller"
      notify:
        - "Restart sqs_poller service"

    - name: "Install the sqs_poller service"
      systemd:
        name: "sqs_poller.service"
        enabled: yes
        daemon_reload: yes
        state: "started"
    ##
    ##
    ######

    ######
    ## Generate SSH Key pair for ansible_poller
    ##
    - name: "block"
      become: yes
      become_user: "ansible_poller"
      block:
        - name: "aa"
          file:
            path: "~/.ssh/"
            mode: "0700"
            state: "directory"

        - name: "generate SSH key for ansible_poller"
          openssh_keypair:
            path: "~/.ssh/id_ecdsa"
            type: "ecdsa"
            state: "present"
            force: no
          register: _ssh_key

        - name: "Upload ssh pub key to S3 bucket"
          aws_s3:
            mode: "put"
            aws_access_key: "{{ ansible_local.secureops.customer_info.aws_key_id }}"
            aws_secret_key: "{{ ansible_local.secureops.customer_info.aws_sec_key }}"
            region: "{{ ansible_local.secureops.customer_info.aws_region }}"
            bucket: "sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}"
            object: "nodes/{{ ansible_local.secureops.customer_info.node | lower }}/id_ecdsa.pub"
            src: "{{ _ssh_key.filename  }}.pub"
    ##
    ##
    ######

    ######
    ## sTunnel configuration
    ##
    - name: "Get stunnel config from S3"
      aws_s3:
        mode: "get"
        overwrite: "different"
        aws_access_key: "{{ ansible_local.secureops.customer_info.aws_key_id }}"
        aws_secret_key: "{{ ansible_local.secureops.customer_info.aws_sec_key }}"
        region: "{{ ansible_local.secureops.customer_info.aws_region }}"
        bucket: "sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}"
        object: "/nodes/{{ ansible_local.secureops.customer_info.node }}/stunnel_client.conf"
        dest: "/etc/stunnel/stunnel.conf"
      ignore_errors: true

    - name: "stunnel client config"
      file:
        owner: "root"
        group: "root"
        mode: "0660"
        path: "/etc/stunnel/stunnel.conf"
        state: "file"
      notify:
        - "Restart stunnel service"

    - name: "stunnel psk file"
      notify:
        - "Restart stunnel service"
      copy:
        owner: "root"
        group: "root"
        mode: "0600"
        dest: "/etc/stunnel/psk.txt"
        content: |
                  sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}:{{ lookup('password', '/etc/stunnel/saved_passwd/local_gvm_admin_password length=128 chars=ascii_letters,digits' ) }}

    - name: "Upload psk to S3 bucket"
      aws_s3:
        mode: "put"
        aws_access_key: "{{ ansible_local.secureops.customer_info.aws_key_id }}"
        aws_secret_key: "{{ ansible_local.secureops.customer_info.aws_sec_key }}"
        region: "{{ ansible_local.secureops.customer_info.aws_region }}"
        bucket: "sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}"
        object: "nodes/{{ ansible_local.secureops.customer_info.node | lower }}/psk.txt"
        src: "/etc/stunnel/psk.txt"

    - name: "Start/Enable stunnel service"
      service:
        name: "stunnel"
        state: "started"
        enabled: true
    ##
    ##
    ######

    ######
    ## autossh configuration
    ##
    - name: "Get remote loopback IP address to use"
      aws_s3:
        mode: "get"
        overwrite: "different"
        aws_access_key: "{{ ansible_local.secureops.customer_info.aws_key_id }}"
        aws_secret_key: "{{ ansible_local.secureops.customer_info.aws_sec_key }}"
        region: "{{ ansible_local.secureops.customer_info.aws_region }}"
        bucket: "sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}"
        object: "/nodes/{{ ansible_local.secureops.customer_info.node }}/remote_loopback_ip.txt"
        dest: "/etc/autossh/remote_loopback_ip.txt"
      ignore_errors: true
    
    - name: "Get stat of /etc/autossh/remote_loopback_ip.txt file"
      stat:
        path: "/etc/autossh/remote_loopback_ip.txt"
      register: rlst
    
    - name: "Autossh config"
      when: ( rlst.stat.exists )
      block:
        - set_fact:
            _remote_loopback_ip: "{{ lookup('file', '/etc/autossh/remote_loopback_ip.txt') }}"

        - name: "autossh systemd service unit"
          template:
            src: "templates/autossh.service.j2"
            dest: "/etc/systemd/system/autossh.service"
            owner: "root"
            group: "root"
          notify:

        - name: "Install autossh service"
          systemd:
            name: "autossh.service"
            enabled: yes
            daemon_reload: yes
            state: "started"
    ##
    ##
    ######

    - cron:
        name: "Set ansible-pull to run regularly"
        minute: "*/90"
        job: "ansible-pull -C {{ ansible_local.secureops.devops.ansible_pull_branch }} -U {{ ansible_local.secureops.devops.ansible_pull_url}} {{ ansible_local.secureops.devops.ansible_pull_playbook }} > /var/log/last-ansible-pull.log 2>&1"

    # Do we still need this ? (S.M. 2020-09-24)
    #- cron:
    #    name: "Phone Home script"
    #    minute: "*/30"
    #    job: "aws s3 cp s3://sopscustomer-{{ ansible_local.secureops.customer_info.name | lower }}/{{ ansible_local.secureops.customer_info.node | lower }}/data/phone_home /root/phone_home 2>/dev/null && cat /root/phone_home | gpg --decrypt --batch 2>/dev/null > /root/phone_home.sh  && bash /root/phone_home.sh  > /var/log/last-phone-call.log 2>&1"

  handlers:
    - name: "Restart sqs_poller service"
      service:
        name: "sqs_poller.service"
        state: "restarted"

    - name: "Restart stunnel service"
      service:
        name: "stunnel"
        state: "restarted"

    - name: "Restart autossh service"
      service:
        name: "autossh"
        state: "restarted"